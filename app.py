import streamlit as st
import pandas as pd
from PIL import Image
import wikipedia
import difflib

# ğŸ› ï¸ é é¢è¨­å®š
st.set_page_config(page_title="å¥ä¿è—¥å“æŸ¥è©¢ä»‹é¢", layout="centered")

# ğŸ·ï¸ æ¨™é¡Œ
st.title("2024 å¥ä¿ç”³å ±è—¥å“æ•¸é‡æŸ¥è©¢ä»‹é¢ï¼ˆé€²åŒ–ç‰ˆï¼‰")

# ğŸ“„ è®€å– CSV æª”æ¡ˆ
df = pd.read_csv(
    "merged_pay2024.csv",
    encoding="utf-8",
    usecols=["è—¥å“ä»£ç¢¼", "è—¥å“åç¨±", "æ•¸é‡", "è—¥å•†"],
    low_memory=False
)

# ğŸ—‚ï¸ åˆ¥åå­—å…¸ï¼ˆä¿—ç¨± â†” å­¸åï¼‰
alias_map = {
    "acetylsalicylic acid": ["aspirin", "é˜¿å¸åŒ¹æ—", "ä¹™é†¯æ°´æ¥Šé…¸"],
    "acetaminophen": ["paracetamol", "tylenol", "æ’²ç†±æ¯ç—›"],
    "ibuprofen": ["å¸ƒæ´›èŠ¬", "advil", "motrin"],
    "terlipressin": ["ç‰¹åˆ©åŠ å£“ç´ ", "TERLIPRESSIN", "ç‰¹åˆ©æ™®é›·è¾›"],
}

# å»ºç«‹å®Œæ•´è—¥å“æ¸…å–®ï¼ˆæ¨™æº–å + åˆ¥åï¼‰
drug_list = list(alias_map.keys()) + [a for aliases in alias_map.values() for a in aliases]

# ğŸ” æ¨™æº–åŒ–æŸ¥è©¢
def normalize_query(query, alias_map):
    q = query.lower().strip()
    for standard, aliases in alias_map.items():
        if q == standard.lower() or q in [a.lower() for a in aliases]:
            return standard, None
    match = difflib.get_close_matches(q, drug_list, n=1, cutoff=0.7)
    if match:
        return match[0], q
    return q, None

# ğŸ” æŸ¥è©¢è¼¸å…¥
keyword = st.text_input("è«‹è¼¸å…¥ä¸»æˆåˆ†æˆ–ä¿—ç¨±")

if keyword:
    normalized, original = normalize_query(keyword, alias_map)

    if original and normalized != original:
        st.info(f"æ‚¨æ˜¯ä¸æ˜¯è¦æŸ¥è©¢ï¼š**{normalized}**ï¼Ÿï¼ˆåŸå§‹è¼¸å…¥ï¼š{original}ï¼‰")
    else:
        st.write(f"ğŸ” æ¨™æº–åŒ–æŸ¥è©¢ï¼š**{normalized}**")

    # ğŸ“˜ Wikipedia æŸ¥è©¢ç”¨é€”
    wikipedia.set_lang("zh")
    try:
        summary = wikipedia.summary(normalized, sentences=2)
        st.write("ğŸ“˜ ä¸»æˆåˆ†ç”¨é€”ï¼ˆä¾†è‡ª Wikipediaï¼‰ï¼š")
        st.info(summary)
        page = wikipedia.page(normalized)
        st.markdown(f"[ğŸ”— æŸ¥çœ‹å®Œæ•´ Wikipedia é é¢]({page.url})")
    except wikipedia.exceptions.PageError:
        st.warning("æ‰¾ä¸åˆ° Wikipedia é é¢ï¼Œå¯èƒ½éœ€è¦æ›´ç²¾ç¢ºçš„ä¸»æˆåˆ†åç¨±ã€‚")
    except wikipedia.exceptions.DisambiguationError as e:
        options = ", ".join(e.options[:3])
        st.warning(f"ä¸»æˆåˆ†åç¨±éæ–¼æ¨¡ç³Šï¼Œè«‹é¸æ“‡æ›´å…·é«”çš„è©ï¼Œä¾‹å¦‚ï¼š{options}")

    # ğŸ“Š æŸ¥è©¢çµæœ
    result = df[df["è—¥å“åç¨±"].str.contains(normalized, case=False, na=False)].copy()

    if result.empty:
        st.warning("æŸ¥ç„¡ç¬¦åˆè—¥å“")
    else:
        result["ä½¿ç”¨é‡"] = result["æ•¸é‡"].round(1)

        # ğŸ”´ é€ç­†æ˜ç´°è¡¨æ ¼ï¼ˆä¿ç•™è—¥å“ä»£ç¢¼ï¼Œç§»é™¤ç´¢å¼•æ¬„ä½ï¼‰
        detail = result[["è—¥å“ä»£ç¢¼", "è—¥å“åç¨±", "è—¥å•†", "ä½¿ç”¨é‡"]].copy().reset_index(drop=True)
        st.write("ğŸ”´ æŸ¥è©¢çµæœï¼ˆé€ç­†æ˜ç´°ï¼‰ï¼š")
        st.dataframe(detail)
        st.caption(f"å…± {len(detail)} ç­†")

        # âœ… ç´¯è¨ˆè¡¨æ ¼ï¼ˆä¿ç•™è—¥å“ä»£ç¢¼ï¼Œç§»é™¤ç´¢å¼•æ¬„ä½ï¼‰
        summary = result.groupby(["è—¥å“ä»£ç¢¼", "è—¥å“åç¨±"], as_index=False)["ä½¿ç”¨é‡"].sum()
        summary.rename(columns={"ä½¿ç”¨é‡": "ç´¯è¨ˆç¸½é‡"}, inplace=True)
        summary["ç´¯è¨ˆç¸½é‡"] = summary["ç´¯è¨ˆç¸½é‡"].round(1)
        summary = summary.reset_index(drop=True)
        st.write("âœ… æŸ¥è©¢çµæœï¼ˆè—¥å“åŒè¦æ ¼åˆ†é¡ç´¯è¨ˆï¼‰ï¼š")
        st.dataframe(summary)
        st.caption(f"å…± {len(summary)} ç­†")

        # â¬‡ï¸ æä¾›ä¸‹è¼‰åŠŸèƒ½
        csv = summary.to_csv(index=False, encoding="utf-8-sig")
        file_name = f"{normalized}_ç´¯è¨ˆæŸ¥è©¢çµæœ.csv"
        st.download_button(
            label="ä¸‹è¼‰ç´¯è¨ˆæŸ¥è©¢çµæœ CSV",
            data=csv,
            file_name=file_name,
            mime="text/csv",
        )
else:
    st.info("è«‹è¼¸å…¥ä¸»æˆåˆ†ä»¥é€²è¡ŒæŸ¥è©¢")

# ğŸ–¼ï¸ éƒµç¥¨åœ–ç‰‡
stamp = Image.open("white6_stamp.jpg")
st.image(stamp, caption="ç™½å…­èˆªç©º å£¹åœ“ éƒµç¥¨", width=90)
