import streamlit as st
import pandas as pd
from PIL import Image
import wikipedia
from rapidfuzz import process

st.set_page_config(page_title="å¥ä¿è—¥å“æŸ¥è©¢ä»‹é¢", layout="centered")

# ğŸ·ï¸ æ¨™é¡Œ
st.title("2024 å¥ä¿ç”³å ±è—¥å“æ•¸é‡æŸ¥è©¢ä»‹é¢ï¼ˆé€²åŒ–ç‰ˆï¼‰")

# ğŸ“„ è®€å– CSV æª”æ¡ˆ
df = pd.read_csv(
    "merged_pay2024.csv",
    encoding="utf-8",
    usecols=["è—¥å“ä»£ç¢¼", "è—¥å“åç¨±", "æ•¸é‡", "è—¥å•†"],
    low_memory=False
)

# ğŸ—‚ï¸ åˆ¥åå­—å…¸
alias_map = {
    "acetaminophen": ["paracetamol", "tylenol", "æ’²ç†±æ¯ç—›"],
    "ibuprofen": ["å¸ƒæ´›èŠ¬", "advil", "motrin"],
    # å¯ä»¥æŒçºŒæ“´å……
}

drug_list = list(alias_map.keys()) + [a for aliases in alias_map.values() for a in aliases]

def normalize_query(query):
    q = query.lower().strip()
    # Step 1: åˆ¥åæ¯”å°
    for standard, aliases in alias_map.items():
        if q == standard or q in [a.lower() for a in aliases]:
            return standard
    # Step 2: æ‹¼å­—ä¿®æ­£
    match = process.extractOne(q, drug_list)
    if match and match[1] > 70:
        return match[0]
    return q  # å¦‚æœæ²’æ‰¾åˆ°ï¼Œå°±ç”¨åŸå§‹è¼¸å…¥

# ğŸ” æŸ¥è©¢è¼¸å…¥
keyword = st.text_input("è«‹è¼¸å…¥ä¸»æˆåˆ†æˆ–ä¿—ç¨±")

if keyword:
    normalized = normalize_query(keyword)
    st.write(f"ğŸ” æ¨™æº–åŒ–æŸ¥è©¢ï¼š**{normalized}**")

    # ğŸ“˜ Wikipedia æŸ¥è©¢ç”¨é€”
    wikipedia.set_lang("zh")
    try:
        summary = wikipedia.summary(normalized, sentences=2)
        st.write("ğŸ“˜ ä¸»æˆåˆ†ç”¨é€”ï¼ˆä¾†è‡ª Wikipediaï¼‰ï¼š")
        st.info(summary)
        page = wikipedia.page(normalized)
        st.markdown(f"[ğŸ”— æŸ¥çœ‹å®Œæ•´ Wikipedia é é¢]({page.url})")
    except wikipedia.exceptions.PageError:
        st.warning("æ‰¾ä¸åˆ° Wikipedia é é¢ï¼Œå¯èƒ½éœ€è¦æ›´ç²¾ç¢ºçš„ä¸»æˆåˆ†åç¨±ã€‚")
    except wikipedia.exceptions.DisambiguationError as e:
        st.warning(f"ä¸»æˆåˆ†åç¨±éæ–¼æ¨¡ç³Šï¼Œè«‹é¸æ“‡æ›´å…·é«”çš„è©ï¼Œä¾‹å¦‚ï¼š{e.options[:3]}")

    # ğŸ“Š æŸ¥è©¢çµæœ
    result = df[df["è—¥å“åç¨±"].str.contains(normalized, case=False, na=False)].copy()

    if result.empty:
        st.warning("æŸ¥ç„¡ç¬¦åˆè—¥å“")
    else:
        result["ä½¿ç”¨é‡"] = result["æ•¸é‡"].round(1)

        # ğŸ”´ é€ç­†æ˜ç´°è¡¨æ ¼
        detail = result[["è—¥å“ä»£ç¢¼", "è—¥å“åç¨±", "è—¥å•†", "ä½¿ç”¨é‡"]].copy()
        detail.insert(0, "åºè™Ÿ", range(1, len(detail) + 1))
        st.write("ğŸ”´ æŸ¥è©¢çµæœï¼ˆé€ç­†æ˜ç´°ï¼‰ï¼š")
        st.dataframe(detail.set_index("åºè™Ÿ"))

        # âœ… ç´¯è¨ˆè¡¨æ ¼
        summary = result.groupby("è—¥å“åç¨±", as_index=False)["ä½¿ç”¨é‡"].sum()
        summary.rename(columns={"ä½¿ç”¨é‡": "ç´¯è¨ˆç¸½é‡"}, inplace=True)
        summary["ç´¯è¨ˆç¸½é‡"] = summary["ç´¯è¨ˆç¸½é‡"].round(1)
        summary.insert(0, "åºè™Ÿ", range(1, len(summary) + 1))
        st.write("âœ… æŸ¥è©¢çµæœï¼ˆè—¥å“åŒè¦æ ¼åˆ†é¡ç´¯è¨ˆï¼‰ï¼š")
        st.dataframe(summary.set_index("åºè™Ÿ"))

        # â¬‡ï¸ æä¾›ä¸‹è¼‰åŠŸèƒ½
        csv = summary.to_csv(index=False, encoding="utf-8-sig")
        file_name = f"{normalized}_ç´¯è¨ˆæŸ¥è©¢çµæœ.csv"
        st.download_button(
            label="ä¸‹è¼‰ç´¯è¨ˆæŸ¥è©¢çµæœ CSV",
            data=csv,
            file_name=file_name,
            mime="text/csv",
        )
else:
    st.info("è«‹è¼¸å…¥ä¸»æˆåˆ†ä»¥é€²è¡ŒæŸ¥è©¢")

# ğŸ–¼ï¸ éƒµç¥¨åœ–ç‰‡
stamp = Image.open("white6_stamp.jpg")
st.image(stamp, caption="ç™½å…­èˆªç©º å£¹åœ“ éƒµç¥¨", width=90)
